{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0377ff4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from transformers import AutoProcessor\n",
    "import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from medclip.modeling_medclip import MedCLIPModel  # Make sure to use the correct MedCLIP import\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, roc_auc_score\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, f1_score, roc_auc_score\n",
    "from tqdm import tqdm\n",
    "from  medclip import  MedCLIPProcessor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "04f91fa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\JARE WORKS\\AppData\\Roaming\\Python\\Python312\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\JARE WORKS\\AppData\\Roaming\\Python\\Python312\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Epoch 1: 100%|██████████| 32/32 [01:10<00:00,  2.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 1.0941\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 32/32 [01:06<00:00,  2.09s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 1.0614\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 32/32 [01:19<00:00,  2.47s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 1.0253\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 32/32 [01:15<00:00,  2.35s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss: 0.9821\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 32/32 [01:09<00:00,  2.18s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 Loss: 0.9345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|██████████| 32/32 [01:09<00:00,  2.17s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 Loss: 0.8839\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|██████████| 32/32 [01:10<00:00,  2.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 Loss: 0.8433\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|██████████| 32/32 [01:10<00:00,  2.19s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 Loss: 0.7958\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 32/32 [01:10<00:00,  2.19s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 Loss: 0.7566\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10: 100%|██████████| 32/32 [01:10<00:00,  2.19s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 Loss: 0.7231\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 32/32 [00:24<00:00,  1.29it/s]\n",
      "c:\\Users\\JARE WORKS\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\JARE WORKS\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\JARE WORKS\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Evaluation Metrics ===\n",
      "Accuracy       : 0.9844\n",
      "F1-Score       : 0.6618\n",
      "AUC-ROC        : 0.6924603174603176\n",
      "Confusion Matrix:\n",
      "[[154   0   0]\n",
      " [  0  98   0]\n",
      " [  3   1   0]]\n",
      "\n",
      "Saved classification_report.csv\n",
      "Saved confusion_matrix.png and loss_curve.png\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, f1_score, roc_auc_score, classification_report\n",
    "from tqdm import tqdm\n",
    "from medclip.modeling_medclip import MedCLIPModel\n",
    "\n",
    "# === Set device to CPU ===\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "# === Load CSV & Encode Labels ===\n",
    "csv_path = \"image_dataset.csv\"\n",
    "df = pd.read_csv(csv_path)\n",
    "label_encoder = LabelEncoder()\n",
    "df['encoded_label'] = label_encoder.fit_transform(df['Classification'])\n",
    "n_classes = df['encoded_label'].nunique()\n",
    "\n",
    "# === Preprocessing ===\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# === Custom Dataset ===\n",
    "class MedClipDataset(Dataset):\n",
    "    def __init__(self, dataframe, image_root, preprocess):\n",
    "        self.dataframe = dataframe\n",
    "        self.image_root = image_root\n",
    "        self.preprocess = preprocess\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.dataframe.iloc[idx]\n",
    "        image_path = os.path.join(self.image_root, row['Image_id'])\n",
    "        label = row['encoded_label']\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        image = self.preprocess(image)\n",
    "        return image, torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "# === Initialize Model ===\n",
    "WEIGHTS_NAME = \"model_weights.pth\"\n",
    "model = MedCLIPModel()\n",
    "model.load_state_dict(torch.load(WEIGHTS_NAME, map_location=device))\n",
    "model = model.to(device)\n",
    "\n",
    "# Attach classifier head\n",
    "with torch.no_grad():\n",
    "    dummy = torch.randn(1, 3, 224, 224).to(device)\n",
    "    hidden_size = model.encode_image(dummy).shape[-1]\n",
    "model.classifier = torch.nn.Linear(hidden_size, n_classes).to(device)\n",
    "\n",
    "# === Dataloader ===\n",
    "image_root = \"BrEaST-Lesions_USG-images_and_masks\"\n",
    "dataset = MedClipDataset(df, image_root, preprocess)\n",
    "dataloader = DataLoader(dataset, batch_size=8, shuffle=True)\n",
    "\n",
    "# === Optimizer & Loss ===\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# === Training ===\n",
    "epochs = 10\n",
    "loss_values = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for images, labels in tqdm(dataloader, desc=f\"Epoch {epoch+1}\"):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        feats = model.encode_image(images)\n",
    "        preds = model.classifier(feats)\n",
    "        loss = criterion(preds, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    loss_values.append(avg_loss)\n",
    "    print(f\"Epoch {epoch+1} Loss: {avg_loss:.4f}\")\n",
    "\n",
    "# === Plot Loss Curve ===\n",
    "plt.figure()\n",
    "plt.plot(range(1, epochs+1), loss_values, marker='o', color='blue')\n",
    "plt.title(\"Training Loss over Epochs\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.grid(True)\n",
    "plt.savefig(\"loss_curve.png\")\n",
    "plt.close()\n",
    "\n",
    "# === Evaluation ===\n",
    "model.eval()\n",
    "y_true, y_pred, y_scores = [], [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in tqdm(dataloader, desc=\"Evaluating\"):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        feats = model.encode_image(images)\n",
    "        preds = model.classifier(feats)\n",
    "        probas = torch.softmax(preds, dim=1)\n",
    "\n",
    "        y_true.extend(labels.cpu().numpy())\n",
    "        y_pred.extend(torch.argmax(probas, dim=1).cpu().numpy())\n",
    "        y_scores.extend(probas.cpu().numpy())\n",
    "\n",
    "# === Metrics ===\n",
    "acc = accuracy_score(y_true, y_pred)\n",
    "f1 = f1_score(y_true, y_pred, average=\"macro\")\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "try:\n",
    "    auc = roc_auc_score(y_true, y_scores, multi_class=\"ovr\")\n",
    "except:\n",
    "    auc = \"AUC Error (Check classes)\"\n",
    "\n",
    "print(\"\\n=== Evaluation Metrics ===\")\n",
    "print(f\"Accuracy       : {acc:.4f}\")\n",
    "print(f\"F1-Score       : {f1:.4f}\")\n",
    "print(f\"AUC-ROC        : {auc}\")\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)\n",
    "\n",
    "# === Classification Report ===\n",
    "report = classification_report(y_true, y_pred, target_names=label_encoder.classes_, output_dict=True)\n",
    "report_df = pd.DataFrame(report).transpose()\n",
    "report_df.to_csv(\"classification_report.csv\")\n",
    "print(\"\\nSaved classification_report.csv\")\n",
    "\n",
    "# === Plot Confusion Matrix ===\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=label_encoder.classes_,\n",
    "            yticklabels=label_encoder.classes_)\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"confusion_matrix.png\")\n",
    "plt.close()\n",
    "print(\"Saved confusion_matrix.png and loss_curve.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "689ac0ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['results/medclip_image_model.joblib']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "joblib.dump(model, \"results/medclip_image_model.joblib\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "048b174d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
